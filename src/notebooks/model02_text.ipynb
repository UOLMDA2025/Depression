{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3dac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import display\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from transformers import BertConfig\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1312533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "PATIENCE = 15\n",
    "DROPOUT = 0.4\n",
    "NUM_EPOCHS = 80\n",
    "LEARNING_RATE = 9e-6\n",
    "MIN_DELTA = 0.005\n",
    "CHUNK_LENGTH = 256\n",
    "CHUNK_STRIDE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ff9830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeder\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d241cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution:\n",
      " 0    77\n",
      "1    30\n",
      "\n",
      "Val label distribution:\n",
      " 0    23\n",
      "1    12\n",
      "\n",
      "Test label distribution:\n",
      " 0    33\n",
      "1    14\n"
     ]
    }
   ],
   "source": [
    "# Load CSVs and build label dicts\n",
    "\n",
    "train_file = pd.read_csv('00_dataset_daicwoz/train_split_Depression_AVEC2017.csv')\n",
    "val_file = pd.read_csv('00_dataset_daicwoz/dev_split_Depression_AVEC2017.csv')\n",
    "test_file = pd.read_csv('00_dataset_daicwoz/full_test_split.csv')\n",
    "\n",
    "train_label_dict = train_file.set_index('Participant_ID')['PHQ8_Binary'].to_dict()\n",
    "val_label_dict = val_file.set_index('Participant_ID')['PHQ8_Binary'].to_dict()\n",
    "test_label_dict = test_file.set_index('Participant_ID')['PHQ_Binary'].to_dict()\n",
    "\n",
    "print(\"Train label distribution:\\n\", pd.Series(list(train_label_dict.values())).value_counts().to_string(index=True))\n",
    "print(\"\\nVal label distribution:\\n\", pd.Series(list(val_label_dict.values())).value_counts().to_string(index=True))\n",
    "print(\"\\nTest label distribution:\\n\", pd.Series(list(test_label_dict.values())).value_counts().to_string(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed796f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:  187\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>interview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>481</td>\n",
       "      <td>Participant: (&lt;synch&gt;). Ellie: (IntroV4Confirm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>407</td>\n",
       "      <td>Participant: (&lt;sync&gt;). Ellie: (IntroV4Confirma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344</td>\n",
       "      <td>Ellie: (hi i'm ellie thanks for coming in toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>393</td>\n",
       "      <td>Participant: (&lt;sync&gt;). Ellie: (IntroV4Confirma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>477</td>\n",
       "      <td>Participant: (&lt;synch&gt;). Ellie: (IntroV4Confirm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Participant_ID                                          interview\n",
       "0             481  Participant: (<synch>). Ellie: (IntroV4Confirm...\n",
       "1             407  Participant: (<sync>). Ellie: (IntroV4Confirma...\n",
       "2             344  Ellie: (hi i'm ellie thanks for coming in toda...\n",
       "3             393  Participant: (<sync>). Ellie: (IntroV4Confirma...\n",
       "4             477  Participant: (<synch>). Ellie: (IntroV4Confirm..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 5595\n",
      "Max length: 31505\n"
     ]
    }
   ],
   "source": [
    "# PUT ALL TRANSCRIPT ANSWERS AS ONE DATAFRAME\n",
    "\n",
    "interviews = []\n",
    "\n",
    "directory = os.fsencode('02_transcripts')\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    \n",
    "    interview = []\n",
    "    transcript_file = pd.read_csv(directory + '/' + filename, sep='\\t', lineterminator='\\r')\n",
    "    for index, row in transcript_file.iterrows():\n",
    "        speaker = row.get('speaker', '')\n",
    "        value = row.get('value', '')\n",
    "        if(speaker and value):\n",
    "            interview.append(f\"{speaker}: ({value})\")\n",
    "    interviews.append([int(filename.split('_')[0]), \". \".join(interview)])\n",
    "\n",
    "df = pd.DataFrame(interviews, columns=['Participant_ID', 'interview'])\n",
    "print('Rows: ', len(df))\n",
    "display(df.head())\n",
    "\n",
    "lengths = [len(data) for _, data in interviews]\n",
    "min_length = min(lengths)\n",
    "max_length = max(lengths)\n",
    "print(\"Min length:\", min_length)\n",
    "print(\"Max length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579e1221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Rows: 106, Val Rows: 35, Test Rows: 46\n"
     ]
    }
   ],
   "source": [
    "# TRAIN & VAL DATAFRAME\n",
    "\n",
    "train_subset = train_file[['Participant_ID', 'PHQ8_Binary']]\n",
    "train_df = pd.merge(train_subset, df, on='Participant_ID', how='inner')\n",
    "\n",
    "val_subset = val_file[['Participant_ID', 'PHQ8_Binary']]\n",
    "val_df = pd.merge(val_subset, df, on='Participant_ID', how='inner')\n",
    "\n",
    "test_subset = test_file[['Participant_ID', 'PHQ_Binary']]\n",
    "test_df = pd.merge(test_subset, df, on='Participant_ID', how='inner')\n",
    "test_df = test_df.rename(columns={'PHQ_Binary': 'PHQ8_Binary'})\n",
    "\n",
    "print(f\"Train Rows: {len(train_df)}, Val Rows: {len(val_df)}, Test Rows: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9ed7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512, stride=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.samples = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            text = row['interview']\n",
    "            label = row['PHQ8_Binary']\n",
    "            participant_id = row['Participant_ID']\n",
    "\n",
    "            # Encode full text (no truncation)\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "            # Chunk the token list\n",
    "            for i in range(0, len(tokens), max_length - stride):\n",
    "                chunk = tokens[i:i + max_length]\n",
    "                if len(chunk) < 10:\n",
    "                    continue\n",
    "                self.samples.append({\n",
    "                    'input_ids': chunk,\n",
    "                    'label': label,\n",
    "                    'participant_id': participant_id\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_ids = sample['input_ids']\n",
    "        label = sample['label']\n",
    "        participant_id = sample['participant_id']\n",
    "\n",
    "        encoding = self.tokenizer.prepare_for_model(\n",
    "            input_ids,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        item['participant_id'] = torch.tensor(participant_id, dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce287df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > MIN_DELTA:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= PATIENCE:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c9e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name, hidden_dropout_prob=DROPOUT, attention_probs_dropout_prob=DROPOUT)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f9beb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker Seed\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = SEED + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f823218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "train_dataset = TextDataset(train_df, tokenizer, CHUNK_LENGTH, CHUNK_STRIDE)\n",
    "val_dataset = TextDataset(val_df, tokenizer, CHUNK_LENGTH, CHUNK_STRIDE)\n",
    "test_dataset = TextDataset(test_df, tokenizer, CHUNK_LENGTH, CHUNK_STRIDE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, worker_init_fn=seed_worker)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426855d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Values\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['PHQ8_Binary']),\n",
    "    y=train_df['PHQ8_Binary']\n",
    ")\n",
    "#class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class_weights = torch.tensor([1.0, 3.0], dtype=torch.float).to(device)\n",
    "print(class_weights);\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.002)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3ae71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Values\n",
    "\n",
    "best_run_val_loss = float('inf')\n",
    "best_run_f1 = 0.0\n",
    "best_run_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e71f5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Train Loss: 0.7071  Val Loss: 0.7067  Val Macro F1: 0.2553  LR: 0.00000900\n",
      "Epoch 2 complete. Train Loss: 0.7043  Val Loss: 0.6870  Val Macro F1: 0.3966  LR: 0.00000900\n",
      "Epoch 3 complete. Train Loss: 0.6990  Val Loss: 0.6929  Val Macro F1: 0.4898  LR: 0.00000900\n",
      "Epoch 4 complete. Train Loss: 0.6788  Val Loss: 0.7655  Val Macro F1: 0.4582  LR: 0.00000900\n",
      "Epoch 5 complete. Train Loss: 0.6660  Val Loss: 0.7683  Val Macro F1: 0.4582  LR: 0.00000900\n",
      "Epoch 6 complete. Train Loss: 0.6514  Val Loss: 0.7410  Val Macro F1: 0.4582  LR: 0.00000900\n",
      "Epoch 7 complete. Train Loss: 0.6251  Val Loss: 0.7524  Val Macro F1: 0.4582  LR: 0.00000630\n",
      "Epoch 8 complete. Train Loss: 0.5990  Val Loss: 0.7461  Val Macro F1: 0.5100  LR: 0.00000630\n",
      "Epoch 9 complete. Train Loss: 0.5855  Val Loss: 0.7587  Val Macro F1: 0.4582  LR: 0.00000630\n",
      "Epoch 10 complete. Train Loss: 0.5565  Val Loss: 0.7660  Val Macro F1: 0.5100  LR: 0.00000630\n",
      "Epoch 11 complete. Train Loss: 0.5303  Val Loss: 0.7741  Val Macro F1: 0.4582  LR: 0.00000441\n",
      "Epoch 12 complete. Train Loss: 0.4999  Val Loss: 0.7901  Val Macro F1: 0.4582  LR: 0.00000441\n",
      "Epoch 13 complete. Train Loss: 0.4947  Val Loss: 0.8274  Val Macro F1: 0.5578  LR: 0.00000441\n",
      "Epoch 14 complete. Train Loss: 0.4738  Val Loss: 0.8170  Val Macro F1: 0.4582  LR: 0.00000441\n",
      "Epoch 15 complete. Train Loss: 0.4564  Val Loss: 0.8249  Val Macro F1: 0.5100  LR: 0.00000309\n",
      "Epoch 16 complete. Train Loss: 0.4271  Val Loss: 0.8451  Val Macro F1: 0.4582  LR: 0.00000309\n",
      "Epoch 17 complete. Train Loss: 0.4326  Val Loss: 0.8939  Val Macro F1: 0.4582  LR: 0.00000309\n",
      "Early stopping triggered at epoch 17\n"
     ]
    }
   ],
   "source": [
    "# Training Code\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'participant_id'}\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    participant_logits = defaultdict(list)\n",
    "    participant_labels = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k not in ['participant_id']}\n",
    "            participant_ids = batch['participant_id']\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "            labels = inputs['labels'].cpu().numpy()\n",
    "\n",
    "            for i in range(len(participant_ids)):\n",
    "                pid = participant_ids[i].item()\n",
    "                participant_logits[pid].append(logits[i])\n",
    "                participant_labels[pid] = labels[i]\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Aggregate logits and compute predictions\n",
    "    final_preds, final_labels = [], []\n",
    "    for pid in participant_logits:\n",
    "        avg_logits = np.mean(participant_logits[pid], axis=0)\n",
    "        pred = np.argmax(avg_logits)\n",
    "        final_preds.append(pred)\n",
    "        final_labels.append(participant_labels[pid])\n",
    "        \n",
    "    # Compute macro F1 score\n",
    "    val_f1 = f1_score(final_labels, final_preds, average='macro')    \n",
    "\n",
    "    print(f\"Epoch {epoch+1} complete. Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}  Val Macro F1: {val_f1:.4f}  LR: {current_lr:.8f}\")\n",
    "\n",
    "    # Save best model of current training\n",
    "    if val_f1 > best_run_f1 or (val_f1 == best_run_f1 and val_loss < best_run_val_loss):\n",
    "        best_run_f1 = val_f1\n",
    "        best_run_val_loss = val_loss\n",
    "        best_run_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Early stopping on val loss\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c90cbb86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating best model from last training run...\n",
      "val_loss: 0.8274\n",
      "AUROC: 0.6051\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73        23\n",
      "           1       0.44      0.33      0.38        12\n",
      "\n",
      "    accuracy                           0.63        35\n",
      "   macro avg       0.57      0.56      0.56        35\n",
      "weighted avg       0.61      0.63      0.61        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of last training\n",
    "\n",
    "if best_run_model_state is not None:\n",
    "    print(\"Evaluating best model from last training run...\")\n",
    "    model.load_state_dict(best_run_model_state)\n",
    "    model.eval()\n",
    "\n",
    "    # Run validation again\n",
    "    participant_logits = defaultdict(list)\n",
    "    participant_labels = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'participant_id'}\n",
    "            participant_ids = batch['participant_id']\n",
    "\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "            labels = inputs['labels'].cpu().numpy()\n",
    "\n",
    "            for i in range(len(participant_ids)):\n",
    "                pid = participant_ids[i].item()\n",
    "                participant_logits[pid].append(probs[i])\n",
    "                participant_labels[pid] = labels[i]\n",
    "\n",
    "    # Aggregate predictions\n",
    "    final_probs, final_preds, final_labels = [], [], []\n",
    "    for pid in participant_logits:\n",
    "        avg_probs = np.mean(participant_logits[pid], axis=0)\n",
    "        final_preds.append(np.argmax(avg_probs))\n",
    "        final_probs.append(avg_probs[1])  # for AUROC\n",
    "        final_labels.append(participant_labels[pid])\n",
    "\n",
    "    # Metrics\n",
    "    val_f1 = f1_score(final_labels, final_preds, average='macro')\n",
    "    auroc = roc_auc_score(final_labels, final_probs)\n",
    "\n",
    "    print(f\"val_loss: {best_run_val_loss:.4f}\")\n",
    "    print(f\"AUROC: {auroc:.4f}\")\n",
    "    print(classification_report(final_labels, final_preds, digits=2))\n",
    "else:\n",
    "    print(\"No best model was found during this run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab00105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_loss: 0.3843\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.94      0.74      0.83        23\n",
    "#            1       0.65      0.92      0.76        12\n",
    "\n",
    "#     accuracy                           0.80        35\n",
    "#    macro avg       0.80      0.83      0.79        35\n",
    "# weighted avg       0.84      0.80      0.81        35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53c10f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model weights.\n",
      "\n",
      "Validation AUROC: 0.6377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70        23\n",
      "           1       0.42      0.42      0.42        12\n",
      "\n",
      "    accuracy                           0.60        35\n",
      "   macro avg       0.56      0.56      0.56        35\n",
      "weighted avg       0.60      0.60      0.60        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation with the best model\n",
    "\n",
    "filename_best = '02_best_text_model.pt'\n",
    "checkpoint = torch.load(filename_best, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded best model weights.\")\n",
    "\n",
    "# Aggregation storage\n",
    "participant_probs = defaultdict(list)\n",
    "participant_labels = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'participant_id'}\n",
    "        participant_ids = batch['participant_id']\n",
    "\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        labels = inputs['labels'].cpu().numpy()\n",
    "\n",
    "        for i in range(len(participant_ids)):\n",
    "            pid = participant_ids[i].item()\n",
    "            participant_probs[pid].append(probs[i])\n",
    "            participant_labels[pid] = labels[i]\n",
    "\n",
    "# Average probabilities per participant\n",
    "final_probs, final_labels = [], []\n",
    "for pid in participant_probs:\n",
    "    avg_prob = np.mean(participant_probs[pid])\n",
    "    final_probs.append(avg_prob)\n",
    "    final_labels.append(participant_labels[pid])\n",
    "\n",
    "# AUROC and classification report\n",
    "auroc = roc_auc_score(final_labels, final_probs)\n",
    "preds = [1 if p >= 0.5 else 0 for p in final_probs]\n",
    "\n",
    "print(f\"\\nValidation AUROC: {auroc:.4f}\")\n",
    "print(classification_report(final_labels, preds, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562df416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded best model weights\n",
    "# Validation AUROC: 0.8333\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.90      0.78      0.84        23\n",
    "#            1       0.67      0.83      0.74        12\n",
    "\n",
    "#     accuracy                           0.80        35\n",
    "#    macro avg       0.78      0.81      0.79        35\n",
    "# weighted avg       0.82      0.80      0.80        35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0579298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUROC: 0.5982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.72        32\n",
      "           1       0.41      0.50      0.45        14\n",
      "\n",
      "    accuracy                           0.63        46\n",
      "   macro avg       0.59      0.59      0.59        46\n",
      "weighted avg       0.65      0.63      0.64        46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Evaluation\n",
    "\n",
    "model.eval()\n",
    "participant_probs = defaultdict(list)\n",
    "participant_labels = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'participant_id'}\n",
    "        participant_ids = batch['participant_id']\n",
    "\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        labels = inputs['labels'].cpu().numpy()\n",
    "\n",
    "        for i in range(len(participant_ids)):\n",
    "            pid = participant_ids[i].item()\n",
    "            participant_probs[pid].append(probs[i])\n",
    "            participant_labels[pid] = labels[i]\n",
    "\n",
    "# Aggregate per participant\n",
    "final_probs, final_labels = [], []\n",
    "for pid in participant_probs:\n",
    "    avg_prob = np.mean(participant_probs[pid])\n",
    "    final_probs.append(avg_prob)\n",
    "    final_labels.append(participant_labels[pid])\n",
    "\n",
    "# Compute AUROC and classification report\n",
    "auroc = roc_auc_score(final_labels, final_probs)\n",
    "preds = [1 if p >= 0.5 else 0 for p in final_probs]\n",
    "\n",
    "print(f\"Test AUROC: {auroc:.4f}\")\n",
    "print(classification_report(final_labels, preds, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a32f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AUROC: 0.5915\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.74      0.72      0.73        32\n",
    "#            1       0.40      0.43      0.41        14\n",
    "\n",
    "#     accuracy                           0.63        46\n",
    "#    macro avg       0.57      0.57      0.57        46\n",
    "# weighted avg       0.64      0.63      0.63        46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a46ae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# Save Best Model\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'best_val_loss': best_run_val_loss,\n",
    "    'best_f1': val_f1,\n",
    "}, filename_best)\n",
    "\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666aed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3a341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
